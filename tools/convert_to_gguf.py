#!/usr/bin/env python3
"""
GGUFå¤‰æ›ãƒ˜ãƒ«ãƒ‘ãƒ¼ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
Future-Code-Ja-8Bãªã©ã€GGUFå½¢å¼ã§æä¾›ã•ã‚Œã¦ã„ãªã„ãƒ¢ãƒ‡ãƒ«ã‚’å¤‰æ›ã™ã‚‹ãŸã‚ã®ãƒ„ãƒ¼ãƒ«
"""
import subprocess
import sys
import os
from pathlib import Path
import shutil
import tempfile

def check_llama_cpp():
    """llama.cppãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª"""
    llama_cpp_path = Path("llama.cpp")
    if not llama_cpp_path.exists():
        print("âŒ llama.cppãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        print("\nä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ã§ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„:")
        print("git clone https://github.com/ggerganov/llama.cpp")
        print("cd llama.cpp")
        print("make  # Linux/macOS")
        print("# ã¾ãŸã¯")
        print("cmake -B build && cmake --build build --config Release  # Windows")
        return False
    return True

def download_model(model_id: str, output_dir: Path):
    """Hugging Faceã‹ã‚‰ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰"""
    print(f"ğŸ“¥ ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­: {model_id}")
    
    # huggingface-hubãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª
    try:
        import huggingface_hub
    except ImportError:
        print("âŒ huggingface-hubãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        print("pip install huggingface-hub")
        return None
    
    try:
        # ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
        from huggingface_hub import snapshot_download
        local_dir = output_dir / model_id.replace("/", "_")
        
        snapshot_download(
            repo_id=model_id,
            local_dir=str(local_dir),
            local_dir_use_symlinks=False
        )
        
        print(f"âœ… ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Œäº†: {local_dir}")
        return local_dir
    except Exception as e:
        print(f"âŒ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {e}")
        return None

def convert_to_gguf(model_path: Path, output_path: Path, quantization: str = "Q4_K_M"):
    """ãƒ¢ãƒ‡ãƒ«ã‚’GGUFå½¢å¼ã«å¤‰æ›"""
    
    llama_cpp_path = Path("llama.cpp")
    
    # ä¸€æ™‚çš„ãªf16ãƒ•ã‚¡ã‚¤ãƒ«
    temp_f16 = output_path.parent / f"{output_path.stem}-f16.gguf"
    
    try:
        # Step 1: HFã‹ã‚‰GGUFï¼ˆf16ï¼‰ã¸ã®å¤‰æ›
        print(f"\nğŸ”„ GGUFå½¢å¼ã«å¤‰æ›ä¸­...")
        convert_script = llama_cpp_path / "convert_hf_to_gguf.py"
        
        if not convert_script.exists():
            # æ–°ã—ã„ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã®llama.cppã§ã¯åå‰ãŒå¤‰ã‚ã£ã¦ã„ã‚‹å¯èƒ½æ€§
            convert_script = llama_cpp_path / "convert.py"
        
        cmd = [
            sys.executable,
            str(convert_script),
            str(model_path),
            "--outfile", str(temp_f16),
            "--outtype", "f16"
        ]
        
        result = subprocess.run(cmd, capture_output=True, text=True)
        if result.returncode != 0:
            print(f"âŒ å¤‰æ›ã‚¨ãƒ©ãƒ¼: {result.stderr}")
            return False
        
        # Step 2: é‡å­åŒ–
        print(f"\nğŸ”„ é‡å­åŒ–ä¸­ ({quantization})...")
        quantize_exe = llama_cpp_path / "llama-quantize"
        if sys.platform == "win32":
            quantize_exe = quantize_exe.with_suffix(".exe")
        
        if not quantize_exe.exists():
            # ãƒ“ãƒ«ãƒ‰ã•ã‚Œã¦ã„ãªã„å¯èƒ½æ€§
            print(f"âŒ {quantize_exe} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚llama.cppã‚’ãƒ“ãƒ«ãƒ‰ã—ã¦ãã ã•ã„ã€‚")
            return False
        
        cmd = [
            str(quantize_exe),
            str(temp_f16),
            str(output_path),
            quantization
        ]
        
        result = subprocess.run(cmd, capture_output=True, text=True)
        if result.returncode != 0:
            print(f"âŒ é‡å­åŒ–ã‚¨ãƒ©ãƒ¼: {result.stderr}")
            return False
        
        print(f"âœ… å¤‰æ›å®Œäº†: {output_path}")
        return True
        
    finally:
        # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
        if temp_f16.exists():
            temp_f16.unlink()

def convert_future_code_ja():
    """Future-Code-Ja-8Bãƒ¢ãƒ‡ãƒ«ã‚’å¤‰æ›"""
    print("ğŸš€ Llama-3.1-Future-Code-Ja-8B GGUFå¤‰æ›ãƒ„ãƒ¼ãƒ«")
    print("=" * 50)
    
    # llama.cppã®ç¢ºèª
    if not check_llama_cpp():
        return
    
    # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
    output_dir = Path("models")
    output_dir.mkdir(exist_ok=True)
    
    # ãƒ¢ãƒ‡ãƒ«IDã¨ãƒ•ã‚¡ã‚¤ãƒ«å
    model_id = "future-architect/Llama-3.1-Future-Code-Ja-8B"
    output_filename = "Llama-3.1-Future-Code-Ja-8B-Q4_K_M.gguf"
    output_path = output_dir / output_filename
    
    # æ—¢ã«å¤‰æ›æ¸ˆã¿ã‹ç¢ºèª
    if output_path.exists():
        print(f"âœ… æ—¢ã«å¤‰æ›æ¸ˆã¿ã§ã™: {output_path}")
        response = input("å†å¤‰æ›ã—ã¾ã™ã‹ï¼Ÿ (y/N): ")
        if response.lower() != 'y':
            return
    
    # ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
    model_path = download_model(model_id, Path("temp_models"))
    if not model_path:
        return
    
    # å¤‰æ›å®Ÿè¡Œ
    print("\n" + "=" * 50)
    print("å¤‰æ›ã‚’é–‹å§‹ã—ã¾ã™ã€‚ã“ã‚Œã«ã¯æ™‚é–“ãŒã‹ã‹ã‚‹å ´åˆãŒã‚ã‚Šã¾ã™...")
    print("å¿…è¦ãªRAM: ç´„16GBä»¥ä¸Š")
    print("å¿…è¦ãªãƒ‡ã‚£ã‚¹ã‚¯å®¹é‡: ç´„20GBä»¥ä¸Š")
    print("=" * 50)
    
    success = convert_to_gguf(model_path, output_path)
    
    # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
    if model_path.parent.name == "temp_models":
        shutil.rmtree(model_path.parent, ignore_errors=True)
    
    if success:
        print(f"\nğŸ‰ å¤‰æ›ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
        print(f"ğŸ“ å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«: {output_path}")
        print(f"ğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {output_path.stat().st_size / (1024**3):.2f} GB")
        print(f"\nThonny Codemateã§ä½¿ç”¨ã§ãã¾ã™ã€‚")
    else:
        print(f"\nâŒ å¤‰æ›ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")

def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    if len(sys.argv) > 1:
        # ã‚«ã‚¹ã‚¿ãƒ ãƒ¢ãƒ‡ãƒ«ã®å¤‰æ›
        if len(sys.argv) < 3:
            print("ä½¿ç”¨æ–¹æ³•: python convert_to_gguf.py <model_path> <output_path> [quantization]")
            print("ä¾‹: python convert_to_gguf.py ./models/my-model ./models/my-model-Q4_K_M.gguf Q4_K_M")
            sys.exit(1)
        
        model_path = Path(sys.argv[1])
        output_path = Path(sys.argv[2])
        quantization = sys.argv[3] if len(sys.argv) > 3 else "Q4_K_M"
        
        if not check_llama_cpp():
            sys.exit(1)
        
        success = convert_to_gguf(model_path, output_path, quantization)
        sys.exit(0 if success else 1)
    else:
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: Future-Code-Ja-8Bã‚’å¤‰æ›
        convert_future_code_ja()

if __name__ == "__main__":
    main()