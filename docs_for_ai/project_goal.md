# ChatGPTから得たOllama/llama.cpp活用メモ（整理版）

## プロジェクトのゴール

- Thonny用にOllamaのプラグインを開発し、GitHub Copilot Agentのように動作させる
- Ollamaサーバを呼び出さず、llama-cpp-pythonでローカルLlamaモデルを実行、
  - Thonnyがロードされたら、モデルを自動的に読み込む
- Agentic codingを実現し、ユーザが指示したコードが生成されるようにする
  - 複数のファイルのコンテキストを活用し、コードの意味を理解して回答・コード生成する
- オプションでChatGPTやOllama、OpenRouterサーバも利用可能にする
- Thonnyとプラグイン、モデルが同梱された状態で、USBメモリに入れ、ファイルをコピーするだけでどこでも動作するようにする
- PyPIに公開し、pip install thonny-ollamaでインストール可能にする

- Thonnyエディタ上で範囲選択したテキストをllamaモデルに渡し、選択範囲のコンテキストを活用したQAを実現
- ユーザのプログラミングスキルを選択可能にし、選択したスキルに応じた回答を生成
- テキストを選択した際に「コード解説」のメニューが表示され、意味を解説してくれる
- Thonnyのエディタ上で選択したテキストをコンテキストとして利用

